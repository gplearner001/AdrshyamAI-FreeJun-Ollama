#!/usr/bin/env python3
"""
WebSocket handler for Teler audio streaming
Implements bidirectional audio streaming between Teler and the application
"""

import json
import logging
import asyncio
import base64
import wave
import io
from typing import Dict, Any, Optional
from fastapi import WebSocket, WebSocketDisconnect
from datetime import datetime, timedelta
from sarvam_service import sarvam_service
from ollama_service import ollama_service
from vad_processor import vad_processor
from database_service import database_service
from webhook_service import webhook_service

logger = logging.getLogger(__name__)

class TelerWebSocketHandler:
    """Handles WebSocket connections and audio streaming with Teler"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.stream_metadata: Dict[str, Dict[str, Any]] = {}
        self.chunk_counter = 1
        self.conversation_history: Dict[str, list] = {}
        self.call_states: Dict[str, Dict[str, Any]] = {}
        self.silence_timers: Dict[str, asyncio.Task] = {}
        self.audio_buffers: Dict[str, list] = {}  # Buffer audio chunks
        self.processing_locks: Dict[str, asyncio.Lock] = {}  # Prevent concurrent processing
        
    async def connect(self, websocket: WebSocket, stream_id: str = None):
        """Accept WebSocket connection and store it"""
        await websocket.accept()
        connection_id = stream_id or f"conn_{datetime.now().timestamp()}"
        self.active_connections[connection_id] = websocket
        self.conversation_history[connection_id] = []
        self.audio_buffers[connection_id] = []
        self.processing_locks[connection_id] = asyncio.Lock()
        
        # Initialize call state
        self.call_states[connection_id] = {
            'status': 'connected',
            'last_user_speech': None,
            'last_ai_response': None,
            'waiting_for_user': True,
            'greeting_sent': False,
            'call_ended': False,
            'silence_warnings': 0,
            'max_silence_warnings': 2,
            'is_processing': False,
            'last_meaningful_speech': None,
            'current_language': 'en-IN',
            'detected_language': None,
            'knowledge_base_id': None
        }
        
        logger.info(f"WebSocket connected: {connection_id}")
        return connection_id
    
    async def _save_call_transcript(self, connection_id: str):
        """Save call transcript to database and send to webhook"""
        try:
            conversation = self.conversation_history.get(connection_id, [])
            call_state = self.call_states.get(connection_id, {})
            stream_metadata = self.stream_metadata.get(connection_id, {})

            if not conversation:
                logger.info(f"No conversation history to save for {connection_id}")
                return

            call_id = stream_metadata.get('call_id', connection_id)

            # Prepare metadata
            metadata = {
                'call_type': 'phone_call',
                'start_time': stream_metadata.get('started_at'),
                'end_time': datetime.now().isoformat(),
                'connection_id': connection_id,
                'from_number': stream_metadata.get('from_number'),
                'to_number': stream_metadata.get('to_number')
            }

            logger.info(f"Saving call transcript for call_id: {call_id}")

            # Save to database
            if database_service.is_available():
                success = database_service.save_call_transcript(
                    call_id=call_id,
                    connection_id=connection_id,
                    conversation=conversation,
                    metadata=metadata,
                    call_state=call_state,
                    stream_metadata=stream_metadata
                )

                if success:
                    logger.info(f"Call transcript saved to database: {call_id}")
                else:
                    logger.warning(f"Failed to save call transcript to database: {call_id}")
            else:
                logger.warning("Database service not available, transcript not saved")

            # Send to webhook if configured
            if webhook_service.is_configured():
                logger.info(f"Sending call transcript to webhook for call_id: {call_id}")
                webhook_success = await webhook_service.send_transcript(
                    call_id=call_id,
                    conversation=conversation,
                    metadata=metadata
                )

                if webhook_success:
                    logger.info(f"Call transcript sent to webhook: {call_id}")
                    # Mark as sent in database
                    if database_service.is_available():
                        database_service.mark_webhook_sent(call_id)
                else:
                    logger.warning(f"Failed to send call transcript to webhook: {call_id}")
            else:
                logger.debug("Webhook not configured, skipping webhook delivery")

        except Exception as e:
            logger.error(f"Error saving call transcript: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def disconnect(self, connection_id: str):
        """Remove WebSocket connection"""
        if connection_id in self.active_connections:
            del self.active_connections[connection_id]

        if connection_id in self.conversation_history:
            del self.conversation_history[connection_id]

        if connection_id in self.stream_metadata:
            del self.stream_metadata[connection_id]

        if connection_id in self.call_states:
            del self.call_states[connection_id]

        if connection_id in self.audio_buffers:
            del self.audio_buffers[connection_id]

        if connection_id in self.processing_locks:
            del self.processing_locks[connection_id]

        # Cancel silence timer if exists
        if connection_id in self.silence_timers:
            self.silence_timers[connection_id].cancel()
            del self.silence_timers[connection_id]

        logger.info(f"WebSocket disconnected: {connection_id}")
    
    async def handle_incoming_message(self, websocket: WebSocket, message: str, connection_id: str):
        """
        Handle incoming messages from Teler
        
        Message types:
        - start: Stream metadata
        - audio: Audio chunk from Teler
        """
        try:
            # Check if call has ended - FIRST CHECK
            call_state = self.call_states.get(connection_id, {})
            if call_state.get('call_ended', False):
                logger.debug(f"Ignoring message for ended call: {connection_id}")
                return
                
            data = json.loads(message)
            message_type = data.get("type")
            
            logger.debug(f"Received WebSocket message type: {message_type} for connection: {connection_id}")
            
            if message_type == "start":
                await self._handle_start_message(data, connection_id)
            elif message_type == "audio":
                await self._handle_audio_message(data, connection_id, websocket)
            else:
                logger.warning(f"Unknown message type: {message_type}")
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON message: {e}")
            logger.error(f"Raw message: {message[:200]}...")
        except Exception as e:
            logger.error(f"Error handling message: {e}")
    
    async def _handle_start_message(self, data: Dict[str, Any], connection_id: str):
        """Handle start message with stream metadata"""
        logger.info(f"Stream started for connection {connection_id}")
        logger.info(f"ЁЯФН FULL START MESSAGE DATA: {json.dumps(data, indent=2)}")

        call_id = data.get("call_id")
        stream_id = data.get("stream_id")
        account_id = data.get("account_id")
        call_app_id = data.get("call_app_id")

        # Extract phone numbers from the start message
        # They can be in the root level or in the data object
        from_number = data.get("from_number") or data.get("data", {}).get("from_number") or data.get("from")
        to_number = data.get("to_number") or data.get("data", {}).get("to_number") or data.get("to")

        logger.info(f"ЁЯФН Extracted IDs - call_id: {call_id}, stream_id: {stream_id}, account_id: {account_id}, call_app_id: {call_app_id}")
        logger.info(f"ЁЯУЮ Extracted Numbers - from_number: {from_number}, to_number: {to_number}")

        # Store stream metadata
        self.stream_metadata[connection_id] = {
            "account_id": account_id,
            "call_app_id": call_app_id,
            "call_id": call_id,
            "stream_id": stream_id,
            "from_number": from_number,
            "to_number": to_number,
            "encoding": data.get("data", {}).get("encoding", "audio/l16"),
            "sample_rate": data.get("data", {}).get("sample_rate", 8000),
            "channels": data.get("data", {}).get("channels", 1),
            "started_at": datetime.now().isoformat()
        }

        logger.info(f"Stream metadata: {self.stream_metadata[connection_id]}")

        # Look up knowledge base ID from call history using multiple identifiers
        knowledge_base_id = self._get_knowledge_base_for_call(call_id, stream_id, account_id)
        if knowledge_base_id:
            logger.info(f"тЬЕ Knowledge base {knowledge_base_id} associated with call {call_id}")
        else:
            logger.warning(f"тЪая╕П No knowledge base found for call {call_id}")

        # Update call state
        if connection_id in self.call_states:
            self.call_states[connection_id]['status'] = 'active'
            self.call_states[connection_id]['knowledge_base_id'] = knowledge_base_id
            logger.info(f"ЁЯУЛ Call state updated - KB ID: {knowledge_base_id}")
        
        # Send initial greeting after a short delay
        await asyncio.sleep(1)  # Give time for connection to stabilize
        await self._send_initial_greeting(connection_id)
        
        # Start silence monitoring
        await self._start_silence_monitoring(connection_id)
    
    async def _handle_audio_message(self, data: Dict[str, Any], connection_id: str, websocket: WebSocket):
        """Handle incoming audio chunk from Teler - BUFFER APPROACH"""
        # Check if call has ended - CRITICAL CHECK
        call_state = self.call_states.get(connection_id, {})
        if call_state.get('call_ended', False):
            logger.debug(f"ЁЯЪл Ignoring audio for ended call: {connection_id}")
            return

        stream_id = data.get("stream_id")
        message_id = data.get("message_id")
        audio_b64 = data.get("data", {}).get("audio_b64")

        if not audio_b64:
            logger.warning("Received audio message without audio data")
            return

        # Decode to check size
        audio_data = base64.b64decode(audio_b64)
        duration_ms = (len(audio_data) / 2) / 8  # 16-bit samples at 8kHz

        logger.debug(f"ЁЯОд Buffering audio chunk {message_id} for stream {stream_id} ({len(audio_data)} bytes, ~{duration_ms:.1f}ms)")

        # Add to audio buffer instead of processing immediately
        if connection_id in self.audio_buffers:
            self.audio_buffers[connection_id].append({
                'audio_b64': audio_b64,
                'message_id': message_id,
                'timestamp': datetime.now(),
                'duration_ms': duration_ms
            })

        # Only process if we're waiting for user input and not already processing
        if (call_state.get('waiting_for_user', True) and
            not call_state.get('is_processing', False)):

            # Check accumulated duration
            accumulated_duration = sum(chunk.get('duration_ms', 0) for chunk in self.audio_buffers[connection_id])

            # Wait until we have at least 3 seconds of audio before processing
            if accumulated_duration >= 3000:  # 3 second minimum
                logger.info(f"тЬЕ Accumulated {accumulated_duration:.0f}ms of audio, processing now...")
                await self._process_accumulated_audio(connection_id, websocket)
    
    async def _process_accumulated_audio(self, connection_id: str, websocket: WebSocket):
        """Process accumulated audio chunks"""
        # Use lock to prevent concurrent processing
        async with self.processing_locks.get(connection_id, asyncio.Lock()):
            call_state = self.call_states.get(connection_id, {})
            
            # Double check call hasn't ended
            if call_state.get('call_ended', False):
                logger.debug(f"ЁЯЪл Call ended during processing: {connection_id}")
                return
                
            # Check if already processing
            if call_state.get('is_processing', False):
                logger.debug(f"тП│ Already processing audio for: {connection_id}")
                return
            
            # Mark as processing
            if connection_id in self.call_states:
                self.call_states[connection_id]['is_processing'] = True
            
            try:
                # Get accumulated audio chunks
                audio_chunks = self.audio_buffers.get(connection_id, [])
                if not audio_chunks:
                    return
                
                # Calculate total duration
                total_duration_ms = sum(chunk.get('duration_ms', 0) for chunk in audio_chunks)
                logger.info(f"ЁЯФД Processing {len(audio_chunks)} accumulated audio chunks for {connection_id} (total: {total_duration_ms:.0f}ms)")

                # Combine audio chunks
                combined_audio = self._combine_audio_chunks(audio_chunks)
                
                # Clear the buffer
                self.audio_buffers[connection_id] = []
                
                # ЁЯОп SPEECH DETECTION: Check if combined audio contains speech using WebRTC VAD
                logger.info(f"ЁЯФН Checking for speech in combined audio using WebRTC VAD...")
                has_speech = vad_processor.has_speech(combined_audio)
                
                if not has_speech:
                    logger.info(f"ЁЯФЗ No speech detected in audio chunk, skipping STT processing for {connection_id}")
                    
                    # Get VAD statistics for debugging
                    vad_stats = vad_processor.get_vad_stats(combined_audio)
                    logger.debug(f"ЁЯУК VAD Stats: {vad_stats}")
                    
                    return  # Skip STT processing for non-speech audio
                
                logger.info(f"ЁЯЧгя╕П Speech detected! Proceeding with STT processing for {connection_id}")
                
                # Optional: Filter audio to keep only speech segments
                filtered_audio = vad_processor.filter_speech_audio(combined_audio)
                if filtered_audio:
                    logger.info(f"ЁЯОп Using filtered speech-only audio for STT")
                    combined_audio = filtered_audio
                else:
                    logger.info(f"тЪая╕П Speech filtering returned empty, using original audio")
                
                # Get current language for this connection
                current_language = self.call_states.get(connection_id, {}).get('current_language', 'en-IN')

                # Process the combined audio with language detection
                stt_result = await self._convert_audio_to_text(combined_audio, connection_id, current_language)

                if stt_result and stt_result.get('transcript') and self._is_meaningful_speech(stt_result['transcript']):
                    transcript = stt_result['transcript']
                    detected_language = stt_result.get('language', current_language)
                    logger.info(f"ЁЯУЭ USER SAID: '{transcript}' (Language: {detected_language}, Connection: {connection_id})")

                    # Check for language switch request
                    switch_language = sarvam_service.detect_language_switch_request(transcript)
                    if switch_language:
                        logger.info(f"ЁЯМР Language switch requested: {current_language} -> {switch_language}")
                        if connection_id in self.call_states:
                            self.call_states[connection_id]['current_language'] = switch_language
                        await self._send_language_switch_confirmation(connection_id, websocket, switch_language)
                        return

                    # Detect language from transcript for auto-switching
                    detected_text_language = await sarvam_service.detect_language_from_text(transcript)
                    if detected_text_language and detected_text_language != current_language:
                        logger.info(f"ЁЯМР Auto language switch detected: {current_language} -> {detected_text_language}")
                        if connection_id in self.call_states:
                            self.call_states[connection_id]['current_language'] = detected_text_language
                            self.call_states[connection_id]['detected_language'] = detected_text_language

                    # Update call state - user has spoken meaningfully
                    if connection_id in self.call_states:
                        self.call_states[connection_id]['last_user_speech'] = datetime.now()
                        self.call_states[connection_id]['last_meaningful_speech'] = transcript
                        self.call_states[connection_id]['waiting_for_user'] = False
                        self.call_states[connection_id]['silence_warnings'] = 0
                    
                    # Add to conversation history
                    if connection_id not in self.conversation_history:
                        self.conversation_history[connection_id] = []
                    
                    self.conversation_history[connection_id].append({
                        "role": "user",
                        "content": transcript
                    })
                    
                    # Generate and send AI response
                    await self._generate_and_send_ai_response(transcript, connection_id, websocket)
                    
                    # Reset silence monitoring
                    await self._reset_silence_monitoring(connection_id)
                else:
                    logger.debug(f"ЁЯФЗ Speech detected but no meaningful transcript generated for {connection_id}")
                    
            except Exception as e:
                logger.error(f"тЭМ Error processing accumulated audio: {e}")
            finally:
                # Mark as not processing
                if connection_id in self.call_states:
                    self.call_states[connection_id]['is_processing'] = False
    
    def _combine_audio_chunks(self, audio_chunks: list) -> str:
        """Combine multiple audio chunks into one"""
        try:
            combined_data = b''
            for chunk in audio_chunks:
                audio_data = base64.b64decode(chunk['audio_b64'])
                combined_data += audio_data

            # Calculate audio characteristics for debugging
            duration_ms = (len(combined_data) / 2) / 8  # 16-bit samples at 8kHz

            # Calculate RMS and peak for audio analysis
            import struct
            samples = struct.unpack(f'<{len(combined_data)//2}h', combined_data)
            rms = (sum(s*s for s in samples) / len(samples)) ** 0.5 if samples else 0
            peak = max(abs(s) for s in samples) if samples else 0

            logger.info(f"ЁЯФН Analyzing combined audio characteristics...")
            logger.info(f"ЁЯУК Audio Info: RMS={rms:.2f}, Peak={peak}, Duration={duration_ms:.1f}ms")

            # Encode back to base64
            return base64.b64encode(combined_data).decode('utf-8')
        except Exception as e:
            logger.error(f"Error combining audio chunks: {e}")
            # Return the first chunk if combination fails
            return audio_chunks[0]['audio_b64'] if audio_chunks else ""
    
    async def _convert_audio_to_text(self, audio_b64: str, connection_id: str, language: str = "en-IN") -> Optional[Dict[str, Any]]:
        """Convert audio to text using Sarvam AI"""
        try:
            logger.info(f"ЁЯОп Converting speech audio to text for connection: {connection_id} (language: {language})")
            logger.debug(f"Audio data length: {len(audio_b64)} base64 characters")

            # Get VAD statistics for logging
            vad_stats = vad_processor.get_vad_stats(audio_b64)
            logger.info(f"ЁЯУК Final VAD Stats before STT: speech_ratio={vad_stats.get('speech_ratio', 0):.2f}, speech_duration={vad_stats.get('speech_duration_ms', 0)}ms")

            # Convert speech to text using Sarvam AI with specified language
            logger.info(f"ЁЯОп Converting speech-validated audio to text with Sarvam AI (language: {language})...")
            stt_result = await sarvam_service.speech_to_text(audio_b64, language=language)

            if stt_result and stt_result.get('transcript') and stt_result['transcript'].strip():
                logger.info(f"ЁЯУЭ STT Result: '{stt_result['transcript']}' (Language: {stt_result.get('language')}, Connection: {connection_id})")
                return {
                    'transcript': stt_result['transcript'].strip(),
                    'language': stt_result.get('language', language)
                }
            else:
                logger.info(f"ЁЯФЗ STT returned empty result despite speech detection for {connection_id}")
                return None
                
        except Exception as e:
            logger.error(f"тЭМ Error converting audio to text: {e}")
            return None
    
    async def _generate_and_send_ai_response(self, user_input: str, connection_id: str, websocket: WebSocket):
        """Generate AI response and send it back"""
        try:
            # Get current language for this connection
            current_language = self.call_states.get(connection_id, {}).get('current_language', 'en-IN')

            # Check if user wants to end the call
            if self._is_end_call_request(user_input):
                logger.info(f"ЁЯЫС User requested to end call: {connection_id} (language: {current_language})")
                logger.info(f"ЁЯУЮ Initiating call termination sequence...")
                await self._end_call_with_goodbye(connection_id, current_language)
                logger.info(f"тЬЕ Call termination sequence completed for {connection_id}")
                return

            # Generate AI response using Claude
            logger.info(f"ЁЯдЦ Generating AI response with Claude (language: {current_language})...")
            ai_response = await self._generate_ai_response(user_input, connection_id)

            if not ai_response:
                # Default fallback based on language
                if current_language == 'en-IN':
                    ai_response = "I understand. Please continue."
                else:
                    ai_response = "рдореИрдВ рд╕рдордЭ рдЧрдпрд╛ред рдХреГрдкрдпрд╛ рдЖрдЧреЗ рдмрддрд╛рдПрдВред"  # Hindi fallback

            logger.info(f"ЁЯТм AI Response: '{ai_response}' (Language: {current_language})")

            # Add AI response to conversation history
            self.conversation_history[connection_id].append({
                "role": "assistant",
                "content": ai_response
            })

            # Get appropriate speaker for language
            speaker = self._get_speaker_for_language(current_language)

            # Convert AI response to speech using Sarvam AI with current language
            logger.info(f"ЁЯФК Converting AI response to speech with Sarvam AI (language: {current_language}, speaker: {speaker})...")
            response_audio = await sarvam_service.text_to_speech(
                text=ai_response,
                language=current_language,
                speaker=speaker
            )
            
            if response_audio:
                await self._send_audio_response(websocket, response_audio)
                logger.info("тЬЕ AI response sent successfully")
                
                # Update call state - now waiting for user again
                if connection_id in self.call_states:
                    self.call_states[connection_id]['waiting_for_user'] = True
                    self.call_states[connection_id]['last_ai_response'] = datetime.now()
            else:
                logger.error("тЭМ Failed to generate response audio")
                
        except Exception as e:
            logger.error(f"тЭМ Error generating and sending AI response: {e}")
    
    async def _send_initial_greeting(self, connection_id: str):
        """Send initial greeting audio to the caller"""
        websocket = self.active_connections.get(connection_id)
        call_state = self.call_states.get(connection_id, {})

        if not websocket or call_state.get('greeting_sent', False) or call_state.get('call_ended', False):
            return

        # Mark greeting as sent
        if connection_id in self.call_states:
            self.call_states[connection_id]['greeting_sent'] = True
            self.call_states[connection_id]['waiting_for_user'] = True

        # Get current language
        current_language = self.call_states.get(connection_id, {}).get('current_language', 'en-IN')

        # Greeting text based on language
        if current_language == 'en-IN':
            greeting_text = "Hello! I am here to help you. Please tell me how I can assist you?"
        else:
            greeting_text = "рдирдорд╕реНрддреЗ! рдореИрдВ рдЖрдкрдХреА рд╕рд╣рд╛рдпрддрд╛ рдХреЗ рд▓рд┐рдП рдпрд╣рд╛рдБ рд╣реВрдБред рдХреГрдкрдпрд╛ рдмрддрд╛рдПрдВ рдХрд┐ рдореИрдВ рдЖрдкрдХреА рдХреИрд╕реЗ рдорджрдж рдХрд░ рд╕рдХрддреА рд╣реВрдБ?"

        # Get appropriate speaker
        speaker = self._get_speaker_for_language(current_language)

        # Generate greeting audio using Sarvam AI TTS
        greeting_audio = await sarvam_service.text_to_speech(
            text=greeting_text,
            language=current_language,
            speaker=speaker
        )
        
        if greeting_audio:
            greeting_message = {
                "type": "audio",
                "audio_b64": greeting_audio,
                "chunk_id": self.chunk_counter
            }
            
            self.chunk_counter += 1
            
            try:
                await websocket.send_text(json.dumps(greeting_message))
                logger.info(f"тЬЕ Sent greeting to connection {connection_id}")
                
                # Update call state
                if connection_id in self.call_states:
                    self.call_states[connection_id]['last_ai_response'] = datetime.now()
                    
            except Exception as e:
                logger.error(f"Failed to send greeting: {e}")
        else:
            logger.warning("Failed to generate greeting audio with Sarvam AI")
    
    def _convert_audio_format(self, audio_b64: str) -> str:
        """Convert Teler audio format to format suitable for Sarvam AI"""
        try:
            # Decode base64 audio
            audio_data = base64.b64decode(audio_b64)
            
            # Create WAV format for Sarvam AI
            wav_buffer = io.BytesIO()
            
            with wave.open(wav_buffer, 'wb') as wav_file:
                wav_file.setnchannels(1)  # Mono
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(8000)  # 8kHz sample rate
                wav_file.writeframes(audio_data)
            
            # Get WAV data and encode to base64
            wav_data = wav_buffer.getvalue()
            wav_b64 = base64.b64encode(wav_data).decode('utf-8')
            
            logger.debug(f"Converted audio: PCM {len(audio_data)} bytes -> WAV {len(wav_data)} bytes")
            return wav_b64
            
        except Exception as e:
            logger.error(f"Error converting audio format: {e}")
            return audio_b64  # Return original if conversion fails
    
    def _is_meaningful_speech(self, transcript: str) -> bool:
        """Check if the transcript contains meaningful speech"""
        if not transcript or not transcript.strip():
            return False
            
        # Remove common filler words and check length
        cleaned = transcript.strip().lower()
        
        # Ignore very short utterances or common fillers
        filler_words = ['so', 'um', 'uh', 'hmm', 'ah', 'er', 'well', 'and', 'the', 'but', 'oh']
        
        # If it's just a filler word, don't consider it meaningful
        if cleaned in filler_words:
            logger.debug(f"Ignoring filler word: '{cleaned}'")
            return False
            
        # If it's very short (less than 4 characters), likely not meaningful
        if len(cleaned) < 4:
            logger.debug(f"Ignoring short utterance: '{cleaned}'")
            return False
            
        # If it's the same word repeated, might be noise
        words = cleaned.split()
        if len(words) == 1 and len(words[0]) < 5:
            logger.debug(f"Ignoring single short word: '{cleaned}'")
            return False
        
        # Check if it's a meaningful sentence (has at least 2 words or one long word)
        if len(words) >= 2 or (len(words) == 1 and len(words[0]) >= 5):
            return True
            
        return False
    
    async def _generate_ai_response(self, user_input: str, connection_id: str) -> Optional[str]:
        """Generate AI response using Ollama based on user input and conversation history."""
        try:
            # Get current language
            current_language = self.call_states.get(connection_id, {}).get('current_language', 'en-IN')

            if not ollama_service.is_available():
                # Fallback responses based on language
                if current_language == 'en-IN':
                    fallback_responses = [
                        "Thank you. What else would you like to know?",
                        "I understand. Please continue.",
                        "That's interesting. What else?",
                        "Okay. What else would you like to say?"
                    ]
                else:
                    fallback_responses = [
                        "рдзрдиреНрдпрд╡рд╛рджред рдЖрдк рдФрд░ рдХреНрдпрд╛ рдЬрд╛рдирдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ?",
                        "рдореИрдВ рд╕рдордЭ рдЧрдпрд╛ред рдХреГрдкрдпрд╛ рдЖрдЧреЗ рдмрддрд╛рдПрдВред",
                        "рдпрд╣ рджрд┐рд▓рдЪрд╕реНрдк рд╣реИред рдФрд░ рдХреНрдпрд╛ рд╣реИ?",
                        "рдЕрдЪреНрдЫрд╛ред рдЖрдк рдФрд░ рдХреНрдпрд╛ рдХрд╣рдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ?"
                    ]
                import random
                return random.choice(fallback_responses)

            # Get knowledge base ID for this call
            knowledge_base_id = self.call_states.get(connection_id, {}).get('knowledge_base_id')
            logger.info(f"Using knowledge base ID: {knowledge_base_id} for AI response")

            conversation_context = {
                'history': self.conversation_history.get(connection_id, []),
                'current_input': user_input,
                'call_id': connection_id,
                'knowledge_base_id': knowledge_base_id,
                'context': {
                    'language': current_language,
                    'mode': 'voice_call',
                    'platform': 'teler',
                    'instruction': 'Keep responses very short (1-2 sentences max). Wait for user to speak. Listen more, talk less.'
                }
            }
            
            response = await ollama_service.generate_conversation_response(conversation_context)
            return response
            
        except Exception as e:
            logger.error(f"Error generating AI response: {str(e)}")
            # Return fallback based on current language
            current_language = self.conversation_history.get(connection_id, [{}])[-1].get('language', 'en-IN') if connection_id in self.conversation_history else 'hi-IN'
            if current_language == 'en-IN':
                return "I'm glad you spoke."
            return "рдореБрдЭреЗ рдЦреБрд╢реА рд╣реИ рдХрд┐ рдЖрдкрдиреЗ рдмрд╛рдд рдХреАред"  # "I'm glad you spoke."
            
    async def _send_audio_response(self, websocket: WebSocket, audio_b64: str):
        """Send audio response back to Teler"""
        response_message = {
            "type": "audio",
            "audio_b64": audio_b64,
            "chunk_id": self.chunk_counter
        }
        
        self.chunk_counter += 1
        
        try:
            await websocket.send_text(json.dumps(response_message))
            logger.debug(f"Sent audio response chunk {self.chunk_counter - 1}")
        except Exception as e:
            logger.error(f"Failed to send audio response: {e}")
    
    async def _start_silence_monitoring(self, connection_id: str):
        """Start monitoring for silence and handle call timeout"""
        if connection_id in self.silence_timers:
            self.silence_timers[connection_id].cancel()
        
        self.silence_timers[connection_id] = asyncio.create_task(
            self._monitor_silence(connection_id)
        )
    
    async def _reset_silence_monitoring(self, connection_id: str):
        """Reset silence monitoring timer"""
        await self._start_silence_monitoring(connection_id)
    
    async def _monitor_silence(self, connection_id: str):
        """Monitor for silence and handle timeouts"""
        try:
            while True:
                await asyncio.sleep(30)  # Check every 30 seconds
                
                call_state = self.call_states.get(connection_id, {})
                if call_state.get('call_ended', False):
                    break
                
                last_speech = call_state.get('last_user_speech')
                
                if not last_speech:
                    continue
                
                # Calculate time since last meaningful user speech
                time_since_speech = datetime.now() - last_speech
                
                # If no speech for 30 seconds, send warning or end call
                if time_since_speech.total_seconds() >= 30:
                    warnings = call_state.get('silence_warnings', 0)
                    max_warnings = call_state.get('max_silence_warnings', 2)
                    
                    if warnings < max_warnings:
                        # Send warning
                        await self._send_silence_warning(connection_id, warnings + 1)
                        if connection_id in self.call_states:
                            self.call_states[connection_id]['silence_warnings'] = warnings + 1
                    else:
                        # End call
                        await self._end_call_gracefully(connection_id)
                        break
                        
        except asyncio.CancelledError:
            logger.debug(f"Silence monitoring cancelled for {connection_id}")
        except Exception as e:
            logger.error(f"Error in silence monitoring: {e}")
    
    def _get_speaker_for_language(self, language: str) -> str:
        """
        Get appropriate speaker for the given language.

        Args:
            language: Language code (e.g., 'en-IN', 'hi-IN')

        Returns:
            Speaker name for Sarvam AI TTS
        """
        speaker_map = {
            'en-IN': 'meera',
            'hi-IN': 'meera',
            'bn-IN': 'meera',
            'gu-IN': 'meera',
            'kn-IN': 'meera',
            'ml-IN': 'meera',
            'mr-IN': 'meera',
            'or-IN': 'meera',
            'pa-IN': 'meera',
            'ta-IN': 'meera',
            'te-IN': 'meera'
        }
        return speaker_map.get(language, 'meera')

    async def _send_language_switch_confirmation(self, connection_id: str, websocket: WebSocket, new_language: str):
        """
        Send confirmation message when language is switched.

        Args:
            connection_id: Connection identifier
            websocket: WebSocket connection
            new_language: New language code
        """
        try:
            # Confirmation messages in different languages
            confirmations = {
                'en-IN': "I will now speak in English. How can I help you?",
                'hi-IN': "рдореИрдВ рдЕрдм рд╣рд┐рдВрджреА рдореЗрдВ рдмреЛрд▓реВрдВрдЧреАред рдореИрдВ рдЖрдкрдХреА рдХреИрд╕реЗ рдорджрдж рдХрд░ рд╕рдХрддреА рд╣реВрдВ?",
                'bn-IN': "ржЖржорж┐ ржПржЦржи ржмрж╛ржВрж▓рж╛ржпрж╝ ржХржерж╛ ржмрж▓ржмред ржЖржорж┐ ржЖржкржирж╛ржХрзЗ ржХрж┐ржнрж╛ржмрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рждрзЗ ржкрж╛рж░рж┐?",
                'gu-IN': "рк╣рлБркВ рк╣рк╡рлЗ ркЧрлБркЬрк░рк╛ркдрлАркорк╛ркВ ркмрлЛрк▓рлАрк╢. рк╣рлБркВ ркдркорк╛рк░рлА ркХрлЗрк╡рлА рк░рлАркдрлЗ ркоркжркж ркХрк░рлА рк╢ркХрлБркВ?",
                'kn-IN': "р▓ир▓╛р▓ир│Б р▓Ир▓Ч р▓Хр▓ир│Нр▓ир▓бр▓жр▓▓р│Нр▓▓р▓┐ р▓ор▓╛р▓др▓ир▓╛р▓бр│Бр▓др│Нр▓др│Зр▓ир│Ж. р▓ир▓╛р▓ир│Б р▓ир▓┐р▓ор▓Чр│Ж р▓╣р│Зр▓Чр│Ж р▓╕р▓╣р▓╛р▓п р▓ор▓╛р▓бр▓мр▓╣р│Бр▓жр│Б?",
                'ml-IN': "р┤Юр┤╛р╡╗ р┤Зр┤кр╡Нр┤кр╡Лр╡╛ р┤ор┤▓р┤пр┤╛р┤│р┤др╡Нр┤др┤┐р╡╜ р┤╕р┤Вр┤╕р┤╛р┤░р┤┐р┤Хр╡Нр┤Хр╡Бр┤В. р┤Юр┤╛р╡╗ р┤ир┤┐р┤Щр╡Нр┤Щр┤│р╡Ж р┤Ор┤Щр╡Нр┤Щр┤ир╡Ж р┤╕р┤╣р┤╛р┤пр┤┐р┤Хр╡Нр┤Хр╡Бр┤В?",
                'mr-IN': "рдореА рдЖрддрд╛ рдорд░рд╛рдареАрдд рдмреЛрд▓реЗрди. рдореА рддреБрдореНрд╣рд╛рд▓рд╛ рдХрд╢реА рдорджрдд рдХрд░реВ рд╢рдХрддреЗ?",
                'or-IN': "рморнБрмБ рммрм░рнНрмдрнНрмдрморм╛рми рмУрмбрм╝рм┐рмЖрм░рнЗ рмХрм╣рм┐рммрм┐ред рморнБрмБ рмЖрмкрмгрмЩрнНрмХрнБ рмХрм┐рмкрм░рм┐ рм╕рм╛рм╣рм╛рмпрнНрнЯ рмХрм░рм┐рмкрм╛рм░рм┐рммрм┐?",
                'pa-IN': "риорйИриВ ри╣рйБриг рикрй░риЬри╛римрйА ри╡ри┐рй▒риЪ римрйЛри▓ри╛риВриЧрйАред риорйИриВ ридрйБри╣ри╛рибрйА риХри┐ри╡рйЗриВ риорижриж риХри░ ри╕риХрижрйА ри╣ри╛риВ?",
                'ta-IN': "роиро╛ройрпН роЗрокрпНрокрпЛродрпБ родрооро┐ро┤ро┐ро▓рпН рокрпЗроЪрпБро╡рпЗройрпН. роиро╛ройрпН роЙроЩрпНроХро│рпБроХрпНроХрпБ роОрокрпНрокроЯро┐ роЙродро╡ роорпБроЯро┐ропрпБроорпН?",
                'te-IN': "р░ир▒Зр░ир▒Б р░Зр░кр▒Нр░кр▒Бр░бр▒Б р░др▒Жр░▓р▒Бр░Чр▒Бр░▓р▒Л р░ор░╛р░Яр▒Нр░▓р░╛р░бр░др░╛р░ир▒Б. р░ир▒Зр░ир▒Б р░ор▒Ар░Хр▒Б р░Ор░▓р░╛ р░╕р░╣р░╛р░пр░В р░Ър▒Зр░пр░Чр░▓р░ир▒Б?"
            }

            confirmation_text = confirmations.get(new_language, confirmations['en-IN'])

            logger.info(f"ЁЯМР Sending language switch confirmation: {new_language}")

            # Get appropriate speaker
            speaker = self._get_speaker_for_language(new_language)

            # Generate confirmation audio
            confirmation_audio = await sarvam_service.text_to_speech(
                text=confirmation_text,
                language=new_language,
                speaker=speaker
            )

            if confirmation_audio:
                confirmation_message = {
                    "type": "audio",
                    "audio_b64": confirmation_audio,
                    "chunk_id": self.chunk_counter
                }

                self.chunk_counter += 1

                await websocket.send_text(json.dumps(confirmation_message))
                logger.info(f"тЬЕ Sent language switch confirmation to {connection_id}")

                # Update call state
                if connection_id in self.call_states:
                    self.call_states[connection_id]['last_ai_response'] = datetime.now()
                    self.call_states[connection_id]['waiting_for_user'] = True

        except Exception as e:
            logger.error(f"Failed to send language switch confirmation: {e}")

    async def _send_silence_warning(self, connection_id: str, warning_number: int):
        """Send a silence warning to the user"""
        websocket = self.active_connections.get(connection_id)
        if not websocket:
            return

        # Get current language
        current_language = self.call_states.get(connection_id, {}).get('current_language', 'en-IN')

        # Multi-language silence warnings
        if warning_number == 1:
            warning_texts = {
                'en-IN': "Are you there? Please speak.",
                'hi-IN': "рдХреНрдпрд╛ рдЖрдк рд╡рд╣рд╛рдБ рд╣реИрдВ? рдХреГрдкрдпрд╛ рдмреЛрд▓реЗрдВред",
                'bn-IN': "ржЖржкржирж┐ ржХрж┐ рж╕рзЗржЦрж╛ржирзЗ ржЖржЫрзЗржи? ржЕржирзБржЧрзНрж░рж╣ ржХрж░рзЗ ржХржерж╛ ржмрж▓рзБржиред",
                'gu-IN': "рк╢рлБркВ ркдркорлЗ ркдрлНркпрк╛ркВ ркЫрлЛ? ркХрлГрккрк╛ ркХрк░рлАркирлЗ ркмрлЛрк▓рлЛ.",
                'kn-IN': "р▓ир│Ар▓╡р│Б р▓Ер▓▓р│Нр▓▓р▓┐р▓жр│Нр▓жр│Ар▓░р▓╛? р▓жр▓пр▓╡р▓┐р▓Яр│Нр▓Яр│Б р▓ор▓╛р▓др▓ир▓╛р▓бр▓┐.",
                'ml-IN': "р┤ир┤┐р┤Щр╡Нр┤Щр╡╛ р┤Ер┤╡р┤┐р┤Яр╡Ж р┤Йр┤гр╡Нр┤Яр╡Л? р┤жр┤пр┤╡р┤╛р┤пр┤┐ р┤╕р┤Вр┤╕р┤╛р┤░р┤┐р┤Хр╡Нр┤Хр╡В.",
                'mr-IN': "рддреБрдореНрд╣реА рддрд┐рдереЗ рдЖрд╣рд╛рдд рдХрд╛? рдХреГрдкрдпрд╛ рдмреЛрд▓рд╛.",
                'or-IN': "рмЖрмкрмг рм╕рнЗрмарм╛рм░рнЗ рмЕрмЫрмирнНрмдрм┐ рмХрм┐? рмжрнЯрм╛рмХрм░рм┐ рмХрнБрм╣рмирнНрмдрнБред",
                'pa-IN': "риХрйА ридрйБри╕рйАриВ риЙриерйЗ ри╣рйЛ? риХри┐ри░рикри╛ риХри░риХрйЗ римрйЛри▓рйЛред",
                'ta-IN': "роирпАроЩрпНроХро│рпН роЕроЩрпНроХрпЗ роЗро░рпБроХрпНроХро┐ро▒рпАро░рпНроХро│ро╛? родропро╡рпБроЪрпЖропрпНродрпБ рокрпЗроЪрпБроЩрпНроХро│рпН.",
                'te-IN': "р░ор▒Ар░░р▒Б р░Ер░Хр▒Нр░Хр░б р░Йр░ир▒Нр░ир░╛р░░р░╛? р░жр░пр░Ър▒Зр░╕р░┐ р░ор░╛р░Яр▒Нр░▓р░╛р░бр░Вр░бр░┐."
            }
        else:
            warning_texts = {
                'en-IN': "I'm waiting for you. Anything else you'd like to say?",
                'hi-IN': "рдореИрдВ рдЖрдкрдХрд╛ рдЗрдВрддрдЬрд╝рд╛рд░ рдХрд░ рд░рд╣рд╛ рд╣реВрдБред рдХреБрдЫ рдФрд░ рдХрд╣рдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ?",
                'bn-IN': "ржЖржорж┐ ржЖржкржирж╛рж░ ржЬржирзНржп ржЕржкрзЗржХрзНрж╖рж╛ ржХрж░ржЫрж┐ред ржЖрж░ ржХрж┐ржЫрзБ ржмрж▓рждрзЗ ржЪрж╛ржи?",
                'gu-IN': "рк╣рлБркВ ркдркорк╛рк░рлА рк░рк╛рк╣ ркЬрлЛркЙркВ ркЫрлБркВ. ркмрлАркЬрлБркВ ркХркВркИ ркХрк╣рлЗрк╡рк╛ ркорк╛ркВркЧрлЛ ркЫрлЛ?",
                'kn-IN': "р▓ир▓╛р▓ир│Б р▓ир▓┐р▓ор▓Чр▓╛р▓Чр▓┐ р▓Хр▓╛р▓пр│Бр▓др│Нр▓др▓┐р▓жр│Нр▓жр│Зр▓ир│Ж. р▓Зр▓ир│Нр▓ир│Зр▓ир▓╛р▓жр▓░р│В р▓╣р│Зр▓│р▓▓р│Б р▓мр▓пр▓╕р│Бр▓др│Нр▓др│Ар▓░р▓╛?",
                'ml-IN': "р┤Юр┤╛р╡╗ р┤ир┤┐р┤Щр╡Нр┤Щр╡╛р┤Хр╡Нр┤Хр┤╛р┤пр┤┐ р┤Хр┤╛р┤др╡Нр┤др┤┐р┤░р┤┐р┤Хр╡Нр┤Хр╡Бр┤ир╡Нр┤ир╡Б. р┤ор┤▒р╡Нр┤▒р╡Жр┤ир╡Нр┤др╡Жр┤Щр╡Нр┤Хр┤┐р┤▓р╡Бр┤В р┤кр┤▒р┤пр┤╛р┤ир╡Бр┤гр╡Нр┤Яр╡Л?",
                'mr-IN': "рдореА рддреБрдордЪреА рд╡рд╛рдЯ рдкрд╛рд╣рдд рдЖрд╣реЗ. рдЖрдгрдЦреА рдХрд╛рд╣реА рд╕рд╛рдВрдЧрд╛рдпрдЪреЗ рдЖрд╣реЗ рдХрд╛?",
                'or-IN': "рморнБрмБ рмЖрмкрмгрмЩрнНрмХ рмкрм╛рмЗрмБ рмЕрмкрнЗрмХрнНрм╖рм╛ рмХрм░рнБрмЫрм┐ред рмЖрмЙ рмХрм┐рмЫрм┐ рмХрм╣рм┐рммрм╛рмХрнБ рмЪрм╛рм╣рнБрмБрмЫрмирнНрмдрм┐ рмХрм┐?",
                'pa-IN': "риорйИриВ ридрйБри╣ри╛рибрйА риЙрибрйАриХ риХри░ ри░ри┐ри╣ри╛ ри╣ри╛риВред риХрйБриЭ ри╣рйЛри░ риХри╣ри┐ригри╛ риЪри╛ри╣рйБрй░рижрйЗ ри╣рйЛ?",
                'ta-IN': "роиро╛ройрпН роЙроЩрпНроХро│рпБроХрпНроХро╛роХ роХро╛родрпНродро┐ро░рпБроХрпНроХро┐ро▒рпЗройрпН. ро╡рпЗро▒рпБ роПродро╛ро╡родрпБ роЪрпКро▓рпНро▓ ро╡ро┐ро░рпБроорпНрокрпБроХро┐ро▒рпАро░рпНроХро│ро╛?",
                'te-IN': "р░ир▒Зр░ир▒Б р░ор▒А р░Хр▒Лр░╕р░В р░Ор░жр▒Бр░░р▒Б р░Ър▒Вр░╕р▒Нр░др▒Бр░ир▒Нр░ир░╛р░ир▒Б. р░Зр░Вр░Хр░╛ р░Пр░жр▒Ир░ир░╛ р░Ър▒Жр░кр▒Нр░кр░╛р░▓р░ир▒Бр░Хр▒Бр░Вр░Яр▒Бр░ир▒Нр░ир░╛р░░р░╛?"
            }

        warning_text = warning_texts.get(current_language, warning_texts['en-IN'])

        logger.info(f"Sending silence warning {warning_number} to {connection_id} in {current_language}")

        # Get appropriate speaker
        speaker = self._get_speaker_for_language(current_language)

        warning_audio = await sarvam_service.text_to_speech(
            text=warning_text,
            language=current_language,
            speaker=speaker
        )

        if warning_audio:
            warning_message = {
                "type": "audio",
                "audio_b64": warning_audio,
                "chunk_id": self.chunk_counter
            }

            self.chunk_counter += 1

            try:
                await websocket.send_text(json.dumps(warning_message))
                logger.info(f"тЬЕ Sent silence warning {warning_number} to {connection_id}")
            except Exception as e:
                logger.error(f"Failed to send silence warning: {e}")
    
    async def _end_call_gracefully(self, connection_id: str):
        """End the call gracefully with a thank you message (auto-timeout)"""
        current_language = self.call_states.get(connection_id, {}).get('current_language', 'en-IN')
        await self._end_call_with_goodbye(connection_id, current_language, reason="inactivity")

    async def _end_call_with_goodbye(self, connection_id: str, language: str = 'en-IN', reason: str = "user_request"):
        """End the call with a goodbye message in the specified language"""
        logger.info(f"ЁЯФЪ _end_call_with_goodbye called for {connection_id} (language: {language}, reason: {reason})")

        websocket = self.active_connections.get(connection_id)
        if not websocket:
            logger.warning(f"тЪая╕П No active websocket found for {connection_id}")
            return

        # Mark call as ended FIRST to prevent any further processing
        logger.info(f"ЁЯЪл Marking call as ended for {connection_id}")
        if connection_id in self.call_states:
            self.call_states[connection_id]['call_ended'] = True
            self.call_states[connection_id]['status'] = 'ended'
            logger.info(f"тЬЕ Call state updated: call_ended={self.call_states[connection_id]['call_ended']}")

        # Save call transcript to database and send to webhook
        logger.info(f"ЁЯТ╛ Saving call transcript for {connection_id}")
        await self._save_call_transcript(connection_id)

        # Multi-language goodbye messages
        farewell_texts = {
            'en-IN': "Thank you for calling. Have a great day. Goodbye!",
            'hi-IN': "рдХреЙрд▓ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рдзрдиреНрдпрд╡рд╛рджред рдЖрдкрдХрд╛ рджрд┐рди рд╢реБрдн рд╣реЛред рдирдорд╕реНрддреЗ!",
            'bn-IN': "ржХрж▓ ржХрж░рж╛рж░ ржЬржирзНржп ржзржирзНржпржмрж╛ржжред ржЖржкржирж╛рж░ ржжрж┐ржи рж╢рзБржн рж╣рзЛржХред ржмрж┐ржжрж╛ржпрж╝!",
            'gu-IN': "ркХрлЙрк▓ ркХрк░рк╡рк╛ ркмркжрк▓ ркЖркнрк╛рк░. ркдркорк╛рк░рлЛ ркжрк┐рк╡рк╕ рк╕рк╛рк░рлЛ рк░рк╣рлЗ. ркЧрлБркбркмрк╛ркп!",
            'kn-IN': "р▓Хр▓░р│Ж р▓ор▓╛р▓бр▓┐р▓жр│Нр▓жр▓Хр│Нр▓Хр▓╛р▓Чр▓┐ р▓зр▓ир│Нр▓пр▓╡р▓╛р▓жр▓Чр▓│р│Б. р▓ир▓┐р▓ор│Нр▓о р▓жр▓┐р▓и р▓╢р│Бр▓нр▓╡р▓╛р▓Чр▓▓р▓┐. р▓╡р▓┐р▓жр▓╛р▓п!",
            'ml-IN': "р┤╡р┤┐р┤│р┤┐р┤Ър╡Нр┤Ър┤др┤┐р┤ир╡Н р┤ир┤ир╡Нр┤жр┤┐. р┤ир┤┐р┤Щр╡Нр┤Щр┤│р╡Бр┤Яр╡Ж р┤жр┤┐р┤╡р┤╕р┤В р┤ир┤▓р╡Нр┤▓р┤др┤╛р┤пр┤┐р┤░р┤┐р┤Хр╡Нр┤Хр┤Яр╡Нр┤Яр╡Ж. р┤╡р┤┐р┤Я!",
            'mr-IN': "рдХреЙрд▓ рдХреЗрд▓реНрдпрд╛рдмрджреНрджрд▓ рдзрдиреНрдпрд╡рд╛рдж. рддреБрдордЪрд╛ рджрд┐рд╡рд╕ рдЪрд╛рдВрдЧрд▓рд╛ рдЬрд╛рд╡реЛ. рдирд┐рд░реЛрдк!",
            'or-IN': "рмХрм▓ рмХрм░рм┐рммрм╛ рмкрм╛рмЗрмБ рмзрмирнНрнЯрммрм╛рмжред рмЖрмкрмгрмЩрнНрмХ рмжрм┐рми рмнрм▓ рм╣рнЗрмЙред рмЧрнБрмбрммрм╛рмЗ!",
            'pa-IN': "риХри╛ри▓ риХри░рии ри▓риИ ризрй░риири╡ри╛рижред ридрйБри╣ри╛рибри╛ рижри┐рии риЪрй░риЧри╛ ри░ри╣рйЗред риЕри▓ри╡ри┐рижри╛!",
            'ta-IN': "роЕро┤рпИродрпНродродро▒рпНроХрпБ роиройрпНро▒ро┐. роЙроЩрпНроХро│рпН роиро╛ро│рпН роЗройро┐родро╛роХ роЕроорпИропроЯрпНроЯрпБроорпН. роХрпБроЯрпНрокрпИ!",
            'te-IN': "р░Хр░╛р░▓р▒Н р░Ър▒Зр░╕р░┐р░ир░Вр░жр▒Бр░Хр▒Б р░зр░ир▒Нр░пр░╡р░╛р░жр░╛р░▓р▒Б. р░ор▒А р░░р▒Лр░Ьр▒Б р░ор░Вр░Ър░┐р░Чр░╛ р░Йр░Вр░бр░╛р░▓р░┐. р░╡р▒Ар░бр▒Нр░Хр▒Лр░▓р▒Б!"
        }

        farewell_text = farewell_texts.get(language, farewell_texts['en-IN'])

        logger.info(f"ЁЯОм Ending call gracefully for {connection_id} (reason: {reason}, language: {language})")
        logger.info(f"ЁЯТм Farewell text: '{farewell_text}'")

        # Get appropriate speaker
        speaker = self._get_speaker_for_language(language)
        logger.info(f"ЁЯЧгя╕П Using speaker: {speaker}")

        logger.info(f"ЁЯФК Generating farewell TTS audio...")
        farewell_audio = await sarvam_service.text_to_speech(
            text=farewell_text,
            language=language,
            speaker=speaker
        )

        if farewell_audio:
            logger.info(f"тЬЕ Farewell audio generated successfully")
            farewell_message = {
                "type": "audio",
                "audio_b64": farewell_audio,
                "chunk_id": self.chunk_counter
            }

            self.chunk_counter += 1

            try:
                await websocket.send_text(json.dumps(farewell_message))
                logger.info(f"тЬЕ Sent farewell message to {connection_id} in {language}")

                # Wait for the message to be sent and played
                # Increased to 5 seconds to ensure non-English messages have time to play
                logger.info(f"тП│ Waiting 5 seconds for farewell audio to play...")
                await asyncio.sleep(5)
                logger.info(f"тЬЕ Farewell audio playback time elapsed, proceeding with connection closure")

            except Exception as e:
                logger.error(f"Failed to send farewell message: {e}")

        # Close the WebSocket connection
        try:
            close_reason = "Call ended due to inactivity" if reason == "inactivity" else "Call ended by user"
            await websocket.close(code=1000, reason=close_reason)
            logger.info(f"тЬЕ Closed WebSocket connection for {connection_id}")
        except Exception as e:
            logger.error(f"Error closing WebSocket: {e}")

        # Clean up all connection resources
        # This ensures proper cleanup even if websocket close doesn't trigger the finally block immediately
        self.disconnect(connection_id)
    
    async def send_interrupt(self, connection_id: str, chunk_id: int):
        """Send interrupt message to stop specific chunk playback"""
        websocket = self.active_connections.get(connection_id)
        if not websocket:
            return
        
        interrupt_message = {
            "type": "interrupt",
            "chunk_id": chunk_id
        }
        
        try:
            await websocket.send_text(json.dumps(interrupt_message))
            logger.info(f"Sent interrupt for chunk {chunk_id}")
        except Exception as e:
            logger.error(f"Failed to send interrupt: {e}")
    
    async def send_clear(self, connection_id: str):
        """Send clear message to wipe out entire buffer"""
        websocket = self.active_connections.get(connection_id)
        if not websocket:
            return
        
        clear_message = {"type": "clear"}
        
        try:
            await websocket.send_text(json.dumps(clear_message))
            logger.info("Sent clear message")
        except Exception as e:
            logger.error(f"Failed to send clear: {e}")
    
    def get_stream_info(self, connection_id: str) -> Optional[Dict[str, Any]]:
        """Get stream metadata for a connection"""
        return self.stream_metadata.get(connection_id)
    
    def get_active_streams(self) -> Dict[str, Dict[str, Any]]:
        """Get all active stream metadata"""
        return self.stream_metadata.copy()

    def _is_end_call_request(self, text: str) -> bool:
        """Check if user wants to end the call"""
        if not text:
            return False

        text_lower = text.lower().strip()
        logger.debug(f"ЁЯФН Checking if text is end call request: '{text_lower}'")

        # End call phrases in multiple languages
        end_phrases = [
            # English
            "goodbye", "bye", "end call", "hang up", "disconnect", "that's all",
            "nothing else", "no more", "i'm done", "thank you bye", "thanks bye",
            "end the call", "finish the call", "bye bye", "good bye",
            # Hindi
            "рдзрдиреНрдпрд╡рд╛рдж", "рдЕрд▓рд╡рд┐рджрд╛", "рдирдорд╕реНрддреЗ", "рдХреЙрд▓ рдЦрддреНрдо рдХрд░реЛ", "рдХреЙрд▓ рдмрдВрдж рдХрд░реЛ",
            "рдмрд╕ рдЗрддрдирд╛ рд╣реА", "рдФрд░ рдХреБрдЫ рдирд╣реАрдВ", "рдХрд╛рдлреА рд╣реИ", "рд╣реЛ рдЧрдпрд╛",
            # Bengali
            "ржзржирзНржпржмрж╛ржж", "ржмрж┐ржжрж╛ржпрж╝", "ржХрж▓ рж╢рзЗрж╖ ржХрж░рзБржи", "ржпржерзЗрж╖рзНржЯ",
            # Gujarati
            "ркЖркнрк╛рк░", "ркЧрлБркбркмрк╛ркп", "ркХрлЙрк▓ ркмркВркз ркХрк░рлЛ", "ркмрк╕",
            # Kannada
            "р▓зр▓ир│Нр▓пр▓╡р▓╛р▓ж", "р▓╡р▓┐р▓жр▓╛р▓п", "р▓Хр▓░р│Ж р▓ор│Бр▓Чр▓┐р▓╕р▓┐", "р▓╕р▓╛р▓Хр│Б",
            # Malayalam
            "р┤ир┤ир╡Нр┤жр┤┐", "р┤╡р┤┐р┤Я", "р┤Хр╡Лр╡╛ р┤Ер┤╡р┤╕р┤╛р┤ир┤┐р┤кр╡Нр┤кр┤┐р┤Хр╡Нр┤Хр╡Бр┤Х", "р┤ор┤др┤┐",
            # Marathi
            "рдзрдиреНрдпрд╡рд╛рдж", "рдирд┐рд░реЛрдк", "рдХреЙрд▓ рдмрдВрдж рдХрд░рд╛", "рдкреБрд░реЗ рдЭрд╛рд▓реЗ",
            # Odia
            "рмзрмирнНрнЯрммрм╛рмж", "рмЧрнБрмбрммрм╛рмЗ", "рмХрм▓рнН рм╢рнЗрм╖ рмХрм░рмирнНрмдрнБ",
            # Punjabi
            "ризрй░риири╡ри╛риж", "риЕри▓ри╡ри┐рижри╛", "риХри╛ри▓ риЦридрио риХри░рйЛ",
            # Tamil
            "роиройрпНро▒ро┐", "роХрпБроЯрпНрокрпИ", "роЕро┤рпИрокрпНрокрпИ роорпБроЯро┐роХрпНроХро╡рпБроорпН", "рокрпЛродрпБроорпН",
            # Telugu
            "р░зр░ир▒Нр░пр░╡р░╛р░жр░╛р░▓р▒Б", "р░╡р▒Ар░бр▒Нр░Хр▒Лр░▓р▒Б", "р░Хр░╛р░▓р▒Н р░ор▒Бр░Чр░┐р░Вр░Ър▒Б", "р░Ър░╛р░▓р▒Б"
        ]

        # Check if any end phrase is in the text
        for phrase in end_phrases:
            if phrase in text_lower:
                logger.info(f"ЁЯЫС DETECTED END CALL PHRASE: '{phrase}' in user text: '{text}'")
                return True

        logger.debug(f"тЬЕ Not an end call request")
        return False

    def _get_knowledge_base_for_call(self, call_id: str, stream_id: str = None, account_id: str = None) -> Optional[str]:
        """Look up knowledge base ID associated with a call using multiple possible identifiers"""
        try:
            from fastapi_app import call_history
            logger.info(f"ЁЯФН Looking up knowledge base for call_id: '{call_id}', stream_id: '{stream_id}', account_id: '{account_id}'")
            logger.info(f"ЁЯУЪ Call history has {len(call_history)} entries")

            if not call_id and not stream_id:
                logger.warning(f"тЪая╕П No identifiers provided for lookup")
                return None

            # Try to find the most recent call with a knowledge base (for Teler calls)
            # Since Teler uses different IDs, we'll use a time-based approach
            # Look for calls initiated in the last 5 minutes with a knowledge base
            from datetime import datetime, timedelta
            recent_threshold = datetime.now() - timedelta(minutes=5)

            for idx, call in enumerate(call_history):
                stored_call_id = call.get('call_id')
                kb_id = call.get('knowledge_base_id')
                call_timestamp = call.get('timestamp')
                call_type = call.get('call_type')

                logger.info(f"  [{idx}] call_id: '{stored_call_id}' | KB: '{kb_id}' | Type: '{call_type}' | Time: {call_timestamp}")

                # Direct match on call_id
                if stored_call_id == call_id:
                    if kb_id:
                        logger.info(f"тЬЕ Found knowledge base '{kb_id}' for call '{call_id}' (direct match)")
                        return kb_id
                    else:
                        logger.info(f"тД╣я╕П Call '{call_id}' found but no knowledge base associated")
                        return None

            # If no direct match, find the most recent "conversation" type call with a KB
            logger.info(f"ЁЯФН No direct match found, looking for recent conversation calls with KB...")
            for idx, call in enumerate(call_history):
                call_type = call.get('call_type')
                kb_id = call.get('knowledge_base_id')
                call_timestamp_str = call.get('timestamp')

                if call_type == 'conversation' and kb_id:
                    try:
                        call_time = datetime.fromisoformat(call_timestamp_str)
                        if call_time >= recent_threshold:
                            logger.info(f"тЬЕ Using knowledge base '{kb_id}' from recent call (within 5 min): {call.get('call_id')}")
                            return kb_id
                    except:
                        pass

            logger.warning(f"тЭМ No matching call found in history for call_id: '{call_id}'")
            logger.warning(f"Available call IDs: {[call.get('call_id') for call in call_history]}")
            return None
        except Exception as e:
            logger.error(f"тЭМ Error looking up knowledge base for call: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

# Global instance
websocket_handler = TelerWebSocketHandler()